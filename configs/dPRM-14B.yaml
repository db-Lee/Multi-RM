model_id: 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B'
task_type: "dPRM"

# ─── Data ────────────────────────────────────────────────────
train_data_path: 'dongboklee/train'
eval_data_path: null
max_length: 650

# ─── Hugging Face Trainer arguments ──────────────────────────
lora_config:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: 'all-linear'
  task_type: 'CAUSAL_LM'

training_args:
  output_dir: null

  # ─ Optimization ─
  learning_rate: 1.0e-4
  num_train_epochs: 1
  effective_batch_size: 16
  lr_scheduler_type: cosine
  warmup_ratio: 0.1
  weight_decay: 0.01
  bf16: true

  # ─ Logging ─
  logging_strategy: steps
  logging_steps: 10

  # ─ Checkpointing ─
  save_strategy: "no"

  # ─ Hub push disabled ─
  push_to_hub: false

  # ─ Reproducibility ─
  seed: 42
  data_seed: 42

  # ─ Misc ─
  use_liger_kernel: true
  gradient_checkpointing: true
  ddp_find_unused_parameters: false